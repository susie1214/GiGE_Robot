# pc_robot_safety_system.py
#
# [ê¸°ëŠ¥ í†µí•©]
# - ... (ì´ì „ê³¼ ë™ì¼) ...
# - [ADD] PC ìì²´ TTS ê¸°ëŠ¥ ì¶”ê°€ (pyttsx3)
#

import socket
import threading
import time
import struct
import cv2
import numpy as np
import torch
import queue
from PIL import Image, ImageDraw, ImageFont
from transformers import AutoModelForCausalLM, AutoTokenizer
from ultralytics import YOLO
import collections
import traceback
import os
from typing import Tuple, Dict
import pyttsx3  # [ADD] PC ìì²´ TTSë¥¼ ìœ„í•œ import

# --- ì „ì—­ ë³€ìˆ˜ ë° ì ê¸ˆ ---
g_latest_frame = None
g_annotated_frame = None
g_robot_motion_command = "FORWARD"
g_robot_speech_command = ""
g_program_running = True

g_frame_lock = threading.Lock()
g_command_lock = threading.Lock()
# ------------------------------------

# _recv_all, video_receive_thread, command_send_thread í•¨ìˆ˜ëŠ” ë³€ê²½ ì‚¬í•­ ì—†ìŒ
# ... (ì´ì „ ì½”ë“œì™€ ë™ì¼) ...

def _recv_all(sock, n):
    """ì†Œì¼“ì—ì„œ n ë°”ì´íŠ¸ë¥¼ ëª¨ë‘ ìˆ˜ì‹ í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    data = bytearray()
    while len(data) < n:
        packet = sock.recv(n - len(data))
        if not packet: return None
        data.extend(packet)
    return data

def video_receive_thread(port):
    """'ëˆˆ' ìŠ¤ë ˆë“œ: RDK X5ì˜ ë¹„ë””ì˜¤ ì—°ê²°ì„ ìˆ˜ì‹ """
    global g_latest_frame, g_program_running, g_annotated_frame
    
    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    server_socket.bind(('0.0.0.0', port)); server_socket.listen(1)
    print(f"ğŸ“¹ ë¹„ë””ì˜¤ ìˆ˜ì‹  ëŒ€ê¸° ì¤‘ (í¬íŠ¸ {port})...")

    while g_program_running:
        conn = None
        try:
            conn, addr = server_socket.accept(); print(f"âœ“ ë¹„ë””ì˜¤ ì—°ê²°ë¨: {addr}")
            
            while g_program_running:
                size_data = _recv_all(conn, 4);
                if size_data is None: break
                msg_size = struct.unpack(">L", size_data)[0]
                frame_data = _recv_all(conn, msg_size)
                if frame_data is None: break
                frame = cv2.imdecode(np.frombuffer(frame_data, dtype=np.uint8), cv2.IMREAD_COLOR)
                with g_frame_lock:
                    g_latest_frame = frame
            
            print(f"âœ— ë¹„ë””ì˜¤ ì—°ê²° ëŠê¹€: {addr}")
            if conn: conn.close()
            with g_frame_lock: g_latest_frame = None; g_annotated_frame = None

        except Exception as e:
            if g_program_running: print(f"âœ— ë¹„ë””ì˜¤ ìˆ˜ì‹  ìŠ¤ë ˆë“œ ì˜¤ë¥˜: {e}")
            if conn: conn.close(); time.sleep(1)
            
    server_socket.close(); print("ë¹„ë””ì˜¤ ìˆ˜ì‹  ìŠ¤ë ˆë“œ ì¢…ë£Œ.")

def command_send_thread(port):
    """'ì…' ìŠ¤ë ˆë“œ: RDK X5ë¡œ 'ë™ì‘|ìŒì„±' ëª…ë ¹ì„ 0.1ì´ˆë§ˆë‹¤ ì „ì†¡"""
    global g_robot_motion_command, g_robot_speech_command, g_program_running

    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    server_socket.bind(('0.0.0.0', port)); server_socket.listen(1)
    print(f"ğŸ¤– ëª…ë ¹ ì „ì†¡ ëŒ€ê¸° ì¤‘ (í¬íŠ¸ {port})...")

    while g_program_running:
        conn = None
        try:
            conn, addr = server_socket.accept(); print(f"âœ“ ëª…ë ¹ ì±„ë„ ì—°ê²°ë¨: {addr}")
            while g_program_running:
                with g_command_lock:
                    motion = g_robot_motion_command
                    speech = g_robot_speech_command
                    g_robot_speech_command = "" # ìŒì„± ëª…ë ¹ì€ í•œ ë²ˆë§Œ ë³´ë‚´ê³  ì´ˆê¸°í™”

                # 'ë™ì‘|ìŒì„±' í¬ë§·ìœ¼ë¡œ ì „ì†¡
                combined_command = f"{motion}|{speech}\n"
                conn.sendall(combined_command.encode('utf-8'))
                time.sleep(0.1) # 1ì´ˆì— 10ë²ˆ ëª…ë ¹ ì „ì†¡

            print(f"âœ— ëª…ë ¹ ì±„ë„ ì—°ê²° ëŠê¹€: {addr}")
            if conn: conn.close()
        except (ConnectionResetError, BrokenPipeError):
            print(f"âœ— ëª…ë ¹ ì±„ë„ í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ëŠê¹€.")
            if conn: conn.close()
        except Exception as e:
            if g_program_running: print(f"âœ— ëª…ë ¹ ì „ì†¡ ìŠ¤ë ˆë“œ ì˜¤ë¥˜: {e}")
            if conn: conn.close(); time.sleep(1)

    server_socket.close(); print("ëª…ë ¹ ì „ì†¡ ìŠ¤ë ˆë“œ ì¢…ë£Œ.")

# ===============================================
# [From safety_watch.py] UI ë° í°íŠ¸ ìœ í‹¸ë¦¬í‹°
# ... (ì´ì „ ì½”ë“œì™€ ë™ì¼) ...
# ===============================================
def get_korean_font(size=20):
    font_paths = [
        "C:\\Windows\\Fonts\\malgun.ttf",
        "/System/Library/Fonts/AppleSDGothicNeo.ttc",
        "/Library/Fonts/AppleSDGothicNeo.ttf",
        "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
        "/usr/share/fonts/truetype/noto/NotoSansKR-Regular.otf",
    ]
    for font_path in font_paths:
        try:
            if os.path.exists(font_path):
                return ImageFont.truetype(font_path, size)
        except: continue
    print("âš ï¸  í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    return ImageFont.load_default()

def put_korean_text(cv_img, text, pos, font_size=20, color=(255, 255, 255)):
    try:
        pil_img = Image.fromarray(cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(pil_img)
        font = get_korean_font(font_size)
        rgb_color = (color[2], color[1], color[0])
        draw.text(pos, text, font=font, fill=rgb_color)
        return cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
    except Exception as e:
        print(f"í•œê¸€ í…ìŠ¤íŠ¸ ë Œë”ë§ ì‹¤íŒ¨: {e}"); return cv_img

def draw_translucent_box(img, box, color=(0, 0, 255), alpha=0.25, thickness=2):
    x1, y1, x2, y2 = map(int, box)
    overlay = img.copy()
    cv2.rectangle(overlay, (x1, y1), (x2, y2), color, -1)
    cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0, img)
    cv2.rectangle(img, (x1, y1), (x2, y2), color, thickness)

def draw_alert_banner(frame, en_text):
    h, w = frame.shape[:2]
    cv2.rectangle(frame, (0, 0), (w, 80), (0, 0, 255), -1) # ë¹¨ê°„ ë°°ê²½
    cv2.putText(frame, en_text, (15, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 255), 3, cv2.LINE_AA) # ë…¸ë€ ê¸€ì”¨
    return frame

def draw_safe_banner(frame, ko_text):
    h, w = frame.shape[:2]
    cv2.rectangle(frame, (0, 0), (w, 80), (128, 128, 128), -1) # íšŒìƒ‰ ë°°ê²½
    frame = put_korean_text(frame, ko_text, (15, 25), font_size=30, color=(0, 255, 0)) # ì´ˆë¡ ê¸€ì”¨
    return frame

def crop_safe(img, box, pad=0):
    h, w = img.shape[:2]
    x1, y1, x2, y2 = box
    x1 = max(0, int(x1 - pad)); y1 = max(0, int(y1 - pad))
    x2 = min(w - 1, int(x2 + pad)); y2 = min(h - 1, int(y2 + pad))
    if x2 <= x1 or y2 <= y1: return None
    return img[y1:y2, x1:x2].copy()

# ===============================================
# [ADD] PC ìì²´ TTS ì›Œì»¤ í´ë˜ìŠ¤ (safety_watch.pyì—ì„œ ë³µì›)
# ===============================================
class TTSWorker:
    """PCì˜ ìŠ¤í”¼ì»¤ë¡œ ì†Œë¦¬ë¥¼ ì¬ìƒí•˜ëŠ” ë¹„ì°¨ë‹¨ TTS ì›Œì»¤"""
    def __init__(self, rate=180, voice_idx=None):
        self.q = queue.Queue()
        try:
            self.eng = pyttsx3.init()
            self.eng.setProperty('rate', rate)
            if voice_idx is not None:
                voices = self.eng.getProperty('voices')
                if 0 <= voice_idx < len(voices):
                    self.eng.setProperty('voice', voices[voice_idx].id)
            self.th = threading.Thread(target=self._run, daemon=True)
            self._last = ""
            self._last_t = 0.0
            self.th.start()
            print("âœ“ PC TTS ì—”ì§„ ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            print(f"âœ— PC TTS ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}. PC ì†Œë¦¬ê°€ ì¬ìƒë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            self.eng = None

    def _run(self):
        while True:
            text = self.q.get()
            try:
                self.eng.say(text)
                self.eng.runAndWait()
            except Exception:
                pass

    def say(self, text: str, cooldown=3.0):
        if not self.eng: # TTS ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ
            return 
            
        now = time.time()
        # ì¿¨ë‹¤ìš´ ë¡œì§ì€ analysis_threadì—ì„œ ì´ë¯¸ ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì œê±°
        # if text != self._last or (now - self._last_t) > cooldown:
        # self._last, self._last_t = text, now
        self.q.put(text)


# ===============================================
# [From safety_watch.py] VQA ëª¨ë¸ ë° ì›Œì»¤ í´ë˜ìŠ¤
# ... (ì´ì „ ì½”ë“œì™€ ë™ì¼) ...
# ===============================================
class MoonVQA:
    def __init__(self, model_id: str = "vikhyatk/moondream2", device: str = None, max_side: int = 448):
        default_device = (
            "cuda" if torch.cuda.is_available()
            else "mps" if torch.backends.mps.is_available()
            else "cpu"
        )
        self.device = device or default_device
        self.max_side = max_side
        self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            dtype=(torch.float16 if self.device == "cuda" else torch.float32),
            trust_remote_code=True
        ).to(self.device).eval()

    @torch.inference_mode()
    def ask(self, image_bgr: np.ndarray, question: str, max_new_tokens: int = 16) -> str:
        if image_bgr is None or image_bgr.size == 0: return ""
        try:
            img_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)
            h, w = img_rgb.shape[:2]
            scale = self.max_side / max(h, w)
            if scale < 1.0:
                img_rgb = cv2.resize(img_rgb, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_AREA)
            
            pil_img = Image.fromarray(img_rgb)
            enc = self.model.encode_image(pil_img)
            
            try: out = self.model.answer_question(enc, question, self.tokenizer, max_new_tokens=max_new_tokens)
            except (AttributeError, TypeError): out = self.model.query(pil_img, question); out = out.get("answer", "") if isinstance(out, dict) else out
            
            return out.strip().lower()
        except Exception as e:
            print(f"[VQA ask ì˜¤ë¥˜] {type(e).__name__}: {e}"); return ""

    def is_yes(self, image_bgr: np.ndarray, question: str) -> bool:
        ans = self.ask(image_bgr, question)
        if "lying" in question.lower():
            positive_keywords = ["yes", "lying", "lying on", "laying", "fallen", "on the floor"]
        elif "helmet" in question.lower():
            positive_keywords = ["yes", "wearing", "wear", "ì°©ìš©"]
        else:
            positive_keywords = ["yes", "true"]

        negative_keywords = ["no", "not", "none", "isn't", "aren't", "ì•„ë‹ˆ", "ì—†"]
        if any(k in ans for k in negative_keywords): return False
        
        result = any(k in ans for k in positive_keywords)
        print(f"[VQA is_yes] Q: {question[:40]}... | A: '{ans}' â†’ {result}")
        return result

class VQAWorker:
    def __init__(self, vqa: MoonVQA, max_cache_size: int = 50):
        self.vqa = vqa
        self.q = queue.Queue(maxsize=8)
        self.out: collections.OrderedDict[int, Tuple[bool, bool]] = collections.OrderedDict()
        self.max_cache_size = max(10, max_cache_size)
        threading.Thread(target=self._run, daemon=True).start()

    def _run(self):
        while True:
            key, person_crop, head_crop = self.q.get()
            fallen_h, fallen_vqa, wearing = False, False, True
            try:
                if person_crop is not None and person_crop.size:
                    h, w = person_crop.shape[:2]; fallen_h = (w / (h + 1e-6)) > 1.8
                    if fallen_h: print(f"[VQA ë¹„ìœ¨ ê°ì§€] Key: {key} (ì“°ëŸ¬ì§ ì˜ì‹¬)")
                    fallen_vqa = self.vqa.is_yes(person_crop, "Is the person lying on the floor? Answer yes or no.")

                if head_crop is not None and head_crop.size:
                    wearing = self.vqa.is_yes(head_crop, "Is the person wearing an industrial safety helmet? Answer yes or no.")
                
                is_fallen = fallen_h or fallen_vqa
                no_helmet = not wearing
                self.out[key] = (is_fallen, no_helmet)
                print(f"[VQA ìµœì¢…] Key: {key} | ì“°ëŸ¬ì§: {is_fallen}, í—¬ë©§ë¯¸ì°©ìš©: {no_helmet}")

                while len(self.out) > self.max_cache_size: self.out.popitem(last=False) 
            except Exception as e:
                print(f"[VQA ì˜¤ë¥˜] Key: {key} | {e}"); self.out[key] = (fallen_h, not wearing)
                while len(self.out) > self.max_cache_size: self.out.popitem(last=False)

    def submit(self, key: int, person_crop: np.ndarray, head_crop: np.ndarray):
        if not self.q.full(): self.q.put((key, person_crop, head_crop))

    def fetch(self, key: int):
        if key in self.out: self.out.move_to_end(key); return self.out[key]
        return None

# ===============================================
# [ë³‘í•©] 'ë‡Œ' ìŠ¤ë ˆë“œ (YOLO + VQA + PC TTS í†µí•©)
# ===============================================
def analysis_thread():
    """'ë‡Œ' ìŠ¤ë ˆë“œ: í™”ì¬(YOLO), ì¥ì• ë¬¼/ì‚¬ëŒ(YOLO), VQA(Moondream) ë™ì‹œ ì²˜ë¦¬ ë° ì œì–´"""
    global g_latest_frame, g_annotated_frame, g_robot_motion_command, g_robot_speech_command, g_program_running

    print("ğŸ§  í†µí•© ë¶„ì„ ìŠ¤ë ˆë“œ ì‹œì‘. AI ëª¨ë¸ ë¡œë“œ ì¤‘...")
    device = (
        "cuda" if torch.cuda.is_available()
        else "mps" if torch.backends.mps.is_available()
        else "cpu"
    )
    try:
        model_fire = YOLO('./weights/firedetect-11s.pt').to(device) 
        model_po = YOLO('yolov8n.pt').to(device)
        vqa = MoonVQA(device=device)
        worker = VQAWorker(vqa)
        
        # [ADD] PC TTS ê°ì²´ ìƒì„±
        pc_tts = TTSWorker(rate=180) 
        
        print(f"âœ“ AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì¥ì¹˜: {device})")
    except Exception as e:
        print(f"âœ— AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}"); g_program_running = False; return

    person_states: Dict[int, Tuple[bool, bool]] = {}
    frame_idx = 0
    vqa_every_n = 8
    vqa_topk = 3
    
    last_fire_alert_time = 0
    last_fallen_alert_time = 0
    last_helmet_alert_time = 0

    while g_program_running:
        frame_copy = None
        with g_frame_lock:
            if g_latest_frame is None: time.sleep(0.05); continue
            frame_copy = g_latest_frame.copy()

        frame_idx += 1
        out_frame = frame_copy.copy()

        # --- 1. AI ì¶”ë¡  (YOLO) ---
        results_fire = model_fire(frame_copy, verbose=False, conf=0.5)
        results_po = model_po(frame_copy, classes=[0, 56, 57, 60], verbose=False, conf=0.5)

        # --- 2. ê°ì§€ ê²°ê³¼ ì¶”ì¶œ ---
        is_fire_detected = len(results_fire[0].boxes) > 0
        is_person_detected = False
        is_obstacle_detected = False
        person_boxes = []

        for b in results_po[0].boxes:
            x1, y1, x2, y2 = map(int, b.xyxy[0].tolist())
            cls = int(b.cls[0])
            if cls == 0:
                is_person_detected = True
                area = max(1, (x2 - x1) * (y2 - y1))
                person_boxes.append(((x1, y1, x2, y2), area))
                draw_translucent_box(out_frame, (x1, y1, x2, y2), (0, 0, 255), alpha=0.1)
            elif cls in [56, 57, 60]:
                is_obstacle_detected = True
                draw_translucent_box(out_frame, (x1, y1, x2, y2), (255, 0, 0), alpha=0.1)
        
        if is_fire_detected:
            for b in results_fire[0].boxes:
                x1, y1, x2, y2 = map(int, b.xyxy[0].tolist())
                draw_translucent_box(out_frame, (x1, y1, x2, y2), (0, 165, 255), alpha=0.4, thickness=3)

        # --- 3. VQA ë¶„ì„ (Script 1 ë¡œì§) ---
        vqa_alerts = []
        detected_indices = set(range(min(len(person_boxes), vqa_topk)))
        current_indices = set(person_states.keys())
        indices_to_remove = current_indices - detected_indices
        for i in indices_to_remove: del person_states[i]

        person_boxes.sort(key=lambda t: t[1], reverse=True)
        
        for i, ((x1, y1, x2, y2), _) in enumerate(person_boxes[:vqa_topk]):
            key = i
            if frame_idx % vqa_every_n == 0:
                person_crop = crop_safe(frame_copy, (x1, y1, x2, y2))
                head_y2 = y1 + max(20, int(0.28 * (y2 - y1)))
                head_crop = crop_safe(frame_copy, (x1, y1, x2, head_y2), pad=4)
                # print(f"[Frame {frame_idx}] VQA ë¶„ì„ ìš”ì²­: Person {key}") # ë¡œê·¸ê°€ ë„ˆë¬´ ë§ì•„ ì£¼ì„ ì²˜ë¦¬
                worker.submit(key, person_crop, head_crop)

            res = worker.fetch(key)
            if res is not None: person_states[i] = res
            
            fallen, no_helmet = person_states.get(i, (False, False))
            
            if fallen: vqa_alerts.append("FALLEN") 
            if no_helmet: vqa_alerts.append("NO HELMET") 

        # --- 4. ìš°ì„ ìˆœìœ„ ì œì–´ ì‹œìŠ¤í…œ ---
        alert_level = "SAFE"
        scene_desc = "ìˆœì°° ì¤‘. ì´ìƒ ì—†ìŒ."
        motion_cmd = "FORWARD"
        speech_cmd = "" # ë¡œë´‡ì—ê²Œ ë³´ë‚¼ ëª…ë ¹
        en_banner_text = ""
        
        now = time.time()

        if is_fire_detected:
            alert_level = "FIRE"
            scene_desc = "í™”ì¬ ê°ì§€! ì¦‰ì‹œ ì •ì§€!"
            motion_cmd = "STOP"
            en_banner_text = "EMERGENCY: FIRE DETECTED!"
            if now - last_fire_alert_time > 5:
                speech_cmd = f"ALERT|{scene_desc}" # ë¡œë´‡ ì „ì†¡ìš©
                pc_tts.say(scene_desc)             # [MOD] PC ì¬ìƒ
                last_fire_alert_time = now

        elif "FALLEN" in vqa_alerts:
            alert_level = "FALLEN"
            scene_desc = "ê¸´ê¸‰ ìƒí™©! ì“°ëŸ¬ì§„ ì‚¬ëŒì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤!"
            motion_cmd = "STOP"
            en_banner_text = "EMERGENCY: PERSON FALLEN!"
            if now - last_fallen_alert_time > 5:
                speech_cmd = f"ALERT|{scene_desc}" # ë¡œë´‡ ì „ì†¡ìš©
                pc_tts.say(scene_desc)             # [MOD] PC ì¬ìƒ
                last_fallen_alert_time = now
            
        elif "NO HELMET" in vqa_alerts:
            alert_level = "NO_HELMET"
            scene_desc = "ê²½ê³ ! ì•ˆì „ëª¨ ë¯¸ì°©ìš©ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤!"
            motion_cmd = "STOP"
            en_banner_text = "WARNING: NO HELMET!"
            if now - last_helmet_alert_time > 5:
                speech_cmd = f"WARN|{scene_desc}"  # ë¡œë´‡ ì „ì†¡ìš©
                pc_tts.say(scene_desc)             # [MOD] PC ì¬ìƒ
                last_helmet_alert_time = now

        elif is_person_detected:
            alert_level = "SAFE"
            scene_desc = "ì‚¬ëŒ í™•ì¸ (ì•ˆì „). ë¡œë´‡ ì •ì§€í•©ë‹ˆë‹¤."
            motion_cmd = "STOP"
        
        elif is_obstacle_detected:
            alert_level = "SAFE"
            scene_desc = "ì¥ì• ë¬¼ ê°ì§€. íšŒí”¼ ê¸°ë™í•©ë‹ˆë‹¤."
            motion_cmd = "TURN_LEFT"
            
        if alert_level == "SAFE":
            last_fire_alert_time = 0
            last_fallen_alert_time = 0
            last_helmet_alert_time = 0

        # --- 5. UI ë°°ë„ˆ ì ìš© ë° ì „ì—­ ë³€ìˆ˜ ì„¤ì • ---
        with g_command_lock:
            g_robot_motion_command = motion_cmd
            if speech_cmd:
                g_robot_speech_command = speech_cmd
        
        if alert_level == "SAFE":
            out_frame = draw_safe_banner(out_frame, scene_desc)
        else:
            # ìœ„í—˜ ë°°ë„ˆì˜ í•œê¸€ ì„¤ëª…ì„ ì˜ë¬¸ìœ¼ë¡œ êµì²´
            out_frame = draw_alert_banner(out_frame, en_banner_text)
            
        with g_frame_lock:
            g_annotated_frame = out_frame

        time.sleep(0.01)

    print("í†µí•© ë¶„ì„ ìŠ¤ë ˆë“œ ì¢…ë£Œ.");

# --- ë©”ì¸ ìŠ¤ë ˆë“œ (GUI í‘œì‹œ) ---
if __name__ == "__main__":
    print("="*50); print("ğŸ’» PC ë¡œë´‡ ì•ˆì „ ê°ì‹œ ì‹œìŠ¤í…œ ì‹œì‘..."); print("="*50)

    threads = [
        threading.Thread(target=video_receive_thread, args=(9999,), daemon=True),
        threading.Thread(target=command_send_thread, args=(9998,), daemon=True),
        threading.Thread(target=analysis_thread, daemon=True),
    ]
    for t in threads: t.start()
        
    print("\nğŸš€ ëª¨ë“  ìŠ¤ë ˆë“œ ì‹œì‘ë¨. 'q' í‚¤ë¥¼ ëˆŒëŸ¬ ì¢…ë£Œ.")
    
    try:
        while g_program_running:
            frame_to_show = None

            with g_frame_lock:
                if g_annotated_frame is not None:
                    frame_to_show = g_annotated_frame.copy() 
                elif g_latest_frame is not None:
                    frame_to_show = g_latest_frame.copy()

            if frame_to_show is not None:
                cv2.imshow("RDK X5 Robot Safety System", frame_to_show)
            else:
                loading_screen = np.zeros((480, 640, 3), dtype=np.uint8)
                cv2.putText(loading_screen, "Waiting for RDK X5 connection...",
                            (50, 240), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
                cv2.imshow("RDK X5 Robot Safety System", loading_screen)

            if cv2.waitKey(33) & 0xFF == ord('q'):
                print("[PC] 'q' í‚¤ ì…ë ¥. ì‹œìŠ¤í…œ ì¢…ë£Œ."); break
                
    except KeyboardInterrupt:
        print("\n[PC] Ctrl+C ê°ì§€. ì‹œìŠ¤í…œ ì¢…ë£Œ...")
    finally:
        g_program_running = False
        print("ëª¨ë“  ìŠ¤ë ˆë“œ ì¢…ë£Œ ì¤‘...");
        for t in threads:
            if t.is_alive(): t.join(timeout=1)
        cv2.destroyAllWindows()
        print("="*50); print("ğŸ’» PC ì œì–´ ì„œë²„ ì¢…ë£Œ."); print("="*50)